{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1400,
   "id": "2078c6da",
   "metadata": {
    "cellId": "z3gcoekpd1qkfce48di5xn"
   },
   "outputs": [
    {
     "ename": "Execute error",
     "evalue": "Servant c1.4 not allocated: Allocation was interrupted",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "%create_livy_session --cluster bigdata-course-spark-cluster \\\n",
    "                     --id ses_test \\\n",
    "                     --conf spark.executor.instances=1 \\\n",
    "                     --conf spark.executor.cores=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1450,
   "id": "43762a2b",
   "metadata": {
    "cellId": "k26bxmmawjej3rnqvo55r"
   },
   "outputs": [],
   "source": [
    "%delete_livy_session  --cluster bigdata-course-spark-cluster-jdrtk8xm --id gr0373-Vlasyuk-Serebryakova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "id": "6f78e6bf",
   "metadata": {
    "cellId": "p1h2hvwv1amzf5qogjzyc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting session gr0373-Vlasyuk-Serebryakova\n"
     ]
    }
   ],
   "source": [
    "%create_livy_session --cluster bigdata-course-spark-cluster-jdrtk8xm \\\n",
    "                     --id gr0373-Vlasyuk-Serebryakova \\\n",
    "                     --conf spark.executor.instances=1 \\\n",
    "                     --conf spark.executor.cores=4\n",
    "#  #!spark —cluster bigdata-course-spark-cluster-11akvxnd —ses.sion gr0000-ivanov-petrov\n",
    "#%delete_livy_session —cluster bigdata-course-spark-cluster-11akvxnd —id gr0000-ivanov-petrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "id": "d355c12c",
   "metadata": {
    "cellId": "pwf9mkcypfqexmxv75rh8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for an Apache Livy session to start...\n",
      "Apache Livy session has started.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "import numpy as np\n",
    "import time\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import PowerIterationClustering\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from functools import reduce\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import random\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "root=r\"/user/bigdata-course/PAMAP2 Physical Activity Monitoring/PAMAP2_Dataset/Protocol/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a080f4",
   "metadata": {
    "cellId": "h4fs6euy8sob3xgh1ev4uw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "id": "bec96746",
   "metadata": {
    "cellId": "7975vwpbh33oi9zrwgu31h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "def load_activity_map():\n",
    "    map = {}\n",
    "    map[0] = 'transient'\n",
    "    map[1] = 'lying'\n",
    "    map[2] = 'sitting'\n",
    "    map[3] = 'standing'\n",
    "    map[4] = 'walking'\n",
    "    map[5] = 'running'\n",
    "    map[6] = 'cycling'\n",
    "    map[7] = 'Nordic_walking'\n",
    "    map[9] = 'watching_TV'\n",
    "    map[10] = 'computer_work'\n",
    "    map[11] = 'car driving'\n",
    "    map[12] = 'ascending_stairs'\n",
    "    map[13] = 'descending_stairs'\n",
    "    map[16] = 'vacuum_cleaning'\n",
    "    map[17] = 'ironing'\n",
    "    map[18] = 'folding_laundry'\n",
    "    map[19] = 'house_cleaning'\n",
    "    map[20] = 'playing_soccer'\n",
    "    map[24] = 'rope_jumping'\n",
    "    return map\n",
    "\n",
    "def generate_three_IMU(name):\n",
    "    x = name +'_x'\n",
    "    y = name +'_y'\n",
    "    z = name +'_z'\n",
    "    return [x,y,z]\n",
    "\n",
    "def generate_four_IMU(name):\n",
    "    x = name +'_x'\n",
    "    y = name +'_y'\n",
    "    z = name +'_z'\n",
    "    w = name +'_w'\n",
    "    return [x,y,z,w]\n",
    "\n",
    "def generate_cols_IMU(name):\n",
    "    # temp\n",
    "    temp = name+'_temperature'\n",
    "    output = [temp]\n",
    "    # acceleration 16\n",
    "    acceleration16 = name+'_3D_acc_16'\n",
    "    acceleration16 = generate_three_IMU(acceleration16)\n",
    "    output.extend(acceleration16)\n",
    "    # acceleration 6\n",
    "    acceleration6 = name+'_3D_acc_6'\n",
    "    acceleration6 = generate_three_IMU(acceleration6)\n",
    "    output.extend(acceleration6)\n",
    "    # gyroscope\n",
    "    gyroscope = name+'_3D_gyro'\n",
    "    gyroscope = generate_three_IMU(gyroscope)\n",
    "    output.extend(gyroscope)\n",
    "    # magnometer\n",
    "    magnometer = name+'_3D_magnetometer'\n",
    "    magnometer = generate_three_IMU(magnometer)\n",
    "    output.extend(magnometer)\n",
    "    # oreintation\n",
    "    oreintation = name+'_4D_orientation'\n",
    "    oreintation = generate_four_IMU(oreintation)\n",
    "    output.extend(oreintation)\n",
    "    return output\n",
    "\n",
    "def load_IMU():\n",
    "    output = ['time_stamp','activity_id', 'heart_rate']\n",
    "    hand = 'hand'\n",
    "    hand = generate_cols_IMU(hand)\n",
    "    output.extend(hand)\n",
    "    chest = 'chest'\n",
    "    chest = generate_cols_IMU(chest)\n",
    "    output.extend(chest)\n",
    "    ankle = 'ankle'\n",
    "    ankle = generate_cols_IMU(ankle)\n",
    "    output.extend(ankle)\n",
    "    return output\n",
    "\n",
    "def load_subjects(root=r\"/user/bigdata-course/PAMAP2 Physical Activity Monitoring/PAMAP2_Dataset/Protocol/\"):\n",
    "    cols = load_IMU()\n",
    "    \n",
    "    path = root + \"*.dat\"\n",
    "    rdd = sc.textFile(path)\n",
    "    numbers_rdd = rdd.map(lambda x : (x.split(\" \"))).filter(lambda x: x[1] != '0')\n",
    "    output = numbers_rdd.toDF(cols)\n",
    "    return output\n",
    "\n",
    "pamap_data = load_subjects()\n",
    "\n",
    "for i in range(1, 10):\n",
    "    #rdd = sc.textFile(path + f\"subject{i}.dat\")\n",
    "    exec(f'subject_{i} = sc.textFile(root + f\"subject10{i}.dat\").map(lambda x : (x.split(\" \"))).filter(lambda x: x[1] != \"0\")')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "index = []\n",
    "values = []\n",
    "for i in range(1, 10):\n",
    "    index.append(f\"subject_{i}\")\n",
    "    values.append(globals()[index[-1]].count())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(index, values)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c0f4a",
   "metadata": {
    "cellId": "mc981xal5ujk4xg4bqfo",
    "execution_id": "d0e456a7-b209-4045-9490-717034f2a8da"
   },
   "source": [
    "## Обрабатывает датасет\n",
    "Аппроксимируем Null и берем части датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "id": "ac4b2db0",
   "metadata": {
    "cellId": "s634e5ci92rc1wgv1v8qfj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "def sum_line(line):  \n",
    "    # Для каждой строки\n",
    "    for i in range(number_of_feature):\n",
    "        # Если значение не NaN, добавить его к сумме для данного столбца\n",
    "        if(line[i]!='NaN'):\n",
    "            sum_of_a_feature[i].add(float(line[i]))\n",
    "\n",
    "def avg_column(accumulator,number_of_row):\n",
    "    # Получить среднее значение для каждого столбца\n",
    "    avg_feature=[]\n",
    "    count=0\n",
    "    for element in sum_of_a_feature:\n",
    "        avg=element.value/number_of_row\n",
    "        value=avg\n",
    "        avg_feature.append(value)\n",
    "        count+=1\n",
    "    return avg_feature\n",
    "\n",
    "def replace_missing_value(line,avg_feature):\n",
    "    # Заменить отсутствующие значения на средние значения по столбцам\n",
    "    for i in range(number_of_feature):\n",
    "        if(line[i]=='NaN'):\n",
    "            line[i]=float(avg_feature[i])\n",
    "        else:\n",
    "            line[i]=float(line[i])\n",
    "    return line\n",
    "\n",
    "def parsePoint(values):\n",
    "    # Преобразовать строку в объект LabeledPoint, используя первый элемент в качестве метки\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "  \n",
    "def process_subject(subject,number_of_feature,sum_of_a_feature):\n",
    "    number_of_row=subject.count()\n",
    "    # Вычислить сумму для каждого столбца\n",
    "    subject.foreach(sum_line)\n",
    "    # Вычислить среднее значение для каждого столбца\n",
    "    avg_feature=avg_column(sum_of_a_feature,number_of_row)\n",
    "    # Заменить отсутствующие значения на средние значения по столбцам\n",
    "    subject_without_missing_value=subject.map(lambda j: replace_missing_value(j,avg_feature))\n",
    "    # Вернуть преобразованный объект\n",
    "    return subject_without_missing_value\n",
    "\n",
    "def insert_label(x):\n",
    "    # Вставить метку в начало списка значений\n",
    "    result=x[1].tolist()\n",
    "    result.insert(0,x[0])\n",
    "    return result\n",
    "\n",
    "number_of_row=subject_8.count()  # Подсчитываем количество строк в датасете subject_8\n",
    "number_of_feature=54  # Устанавливаем количество признаков для датасета\n",
    "sum_of_a_feature = [sc.accumulator(0) for x in range(number_of_feature)]  # Создаем список с аккумуляторами для каждого признака\n",
    "\n",
    "subject_rdd_list = []  # Создаем пустой список для хранения RDD датасетов\n",
    "for i in range(1, 9):\n",
    "    #exec(f'subject{i} = process_subject(subject_{i},number_of_feature,sum_of_a_feature)')\n",
    "    exec(f'subject{i} = process_subject(subject_{i},number_of_feature,sum_of_a_feature).map(lambda x:(((x[1],int(x[0]/5)),(np.asarray(x[2:]),1)))).reduceByKey(lambda a,b: (np.sum([a[0],b[0]], axis=0),a[1]+b[1])).map(lambda x: ((x[0][0]),np.true_divide(x[1][0],x[1][1]))).map(insert_label)')\n",
    "    subject_rdd_list.append(globals()[f'subject{i}'])  # Добавляем каждый RDD датасета subject_i в список, используя глобальное имя переменной\n",
    "    #rdd = sc.textbject{i} = process_subject(subject_{i},number_of_feature,sum_of_a_feature)')\n",
    "   # exec(f'subject{i} = process_subject(subject_{i},number_of_feature,sum_of_a_feature).map(lambda x:(((x[1],int(x[0]/5)),(np.asarray(x[2:]),1)))).reduceByKey(lambda a,b: (np.sum([a[0],b[0]], axis=0),a[1]+b[1])).map(lambda x: ((x[0][0]),np.true_divide(x[1][0],x[1][1]))).map(insert_label)')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1103fd8",
   "metadata": {
    "cellId": "xy1o7a11xpejuskkqkypsg"
   },
   "source": [
    "начинаем кластеризацию "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1444,
   "id": "01462c16",
   "metadata": {
    "cellId": "x5d9ujmyaffl090fsvhm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.feature.StandardScalerModel object at 0x7f2f218d97f0>\n",
      "MapPartitionsRDD[15018] at mapPartitions at PythonMLLibAPI.scala:1342\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "rdd = sc.union(subject_rdd_list)\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(rdd)\n",
    "normalizedData = scalerModel.transform(rdd)\n",
    "\n",
    "print(scalerModel)\n",
    "\n",
    "print(normalizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1455,
   "id": "f12ab0c0",
   "metadata": {
    "cellId": "6at7llgzs9jkikv7i4v1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      0|   69|\n",
      "|      1| 1915|\n",
      "|      2|  187|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----------+-----+\n",
      "|cluster|activity_id|count|\n",
      "+-------+-----------+-----+\n",
      "|      0|          1|   69|\n",
      "|      2|          1|  165|\n",
      "|      1|          1| 1657|\n",
      "|      2|         24|   22|\n",
      "|      1|         24|  258|\n",
      "+-------+-----------+-----+\n",
      "\n",
      "Execution time of the program is-  4.175935983657837\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "rdd = sc.union(subject_rdd_list)\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(rdd)\n",
    "normalizedData = scalerModel.transform(rdd)\n",
    "\n",
    "#starting time\n",
    "start = time.time()\n",
    "clusters = KMeans.train(normalizedData, 3, maxIterations=10, initializationMode=\"random\")\n",
    "end = time.time()\n",
    "# print(\"Cluster centers:\")\n",
    "# for center in clusters.centers:\n",
    "#     print(center)\n",
    "    \n",
    "predictions = rdd.map(lambda x: clusters.predict(x))\n",
    "\n",
    "\n",
    "\n",
    "pred_df = predictions.map(lambda x: (x, )).toDF([\"cluster\"])\n",
    "pred_df = pred_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Добавляем индекс к исходному DataFrame и объединяем по индексу\n",
    "apr_df_id = pamap_data.withColumn(\"id\", monotonically_increasing_id())\n",
    "result_df = apr_df_id.join(pred_df, on=\"id\", how=\"inner\").drop(\"id\")\n",
    "\n",
    "# вывод количества записей в каждом кластере\n",
    "result_df.groupBy(\"cluster\").count().show()\n",
    "\n",
    "# вывод типов активности в каждом кластере\n",
    "result_df.groupBy(\"cluster\", \"activity_id\").count().orderBy([\"activity_id\"]).show(20)\n",
    "\n",
    "print(\"Execution time of the program is- \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1456,
   "id": "35817de7",
   "metadata": {
    "cellId": "khrc1xg5uyh9mqemwh1896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 462178.0808064495\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "\n",
    "\"\"\"\n",
    "WSSSE (Within Set Sum of Squared Error) - это метрика, используемая для оценки качества модели кластеризации. \n",
    "Она измеряет сумму квадратов расстояний между каждой точкой и центром кластера, к которому эта точка относится. \n",
    "Чем меньше WSSSE, тем лучше качество кластеризации.\n",
    "\"\"\"\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]  # Находим центр кластера, к которому относится точка\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))  # Вычисляем расстояние между точкой и центром кластера\n",
    "\n",
    "WSSSE = rdd.map(lambda point: error(point)).reduce(lambda x, y: x + y)  # Вычисляем сумму квадратов ошибок для каждой точки и суммируем их\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))  # Выводим сумму квадратов ошибок на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "id": "3262a51a",
   "metadata": {
    "cellId": "yv8j6q5lz2ycb7g6a5xdg",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-a7bdcd1046a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecute_livy_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://10.128.0.22:8998/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gr0373-Vlasyuk-Serebryakova'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n\\nimport numpy as np\\n\\n# Вычисляем расстояние между точками и центроидами кластеров\\ndef distance(point, center):\\n    return np.linalg.norm(point - center)\\n\\ndef calculate_distances(point, centers):\\n    return [distance(point, center) for center in centers]\\n\\n# Вычисляем среднее расстояние до точек внутри кластера\\ndef avg_distance_in_cluster(point, cluster_points):\\n    point = np.array(point)\\n    cluster_points = [np.array(p) for p in cluster_points]\\n    distances = [distance(point, p) for p in cluster_points if not np.array_equal(p, point)]\\n    return sum(distances) / len(distances) if distances else 0.0\\n\\ndef calculate_avg_distances(points, center, clusters):\\n    points = [np.array(p) for p in points]\\n    cluster_points = [p for p, c in zip(points, clusters) if c == center]\\n    return [avg_distance_in_cluster(point, cluster_points) for point in cluster_points]\\n\\n# Вычисляем Silhouette Score\\ndef silhouette_score(points, clusters):\\n    n = points.count()\\n    s = 0.0\\n    for point, cluster in zip(points.collect(), clusters.collect()):\\n        a = avg_distance_in_cluster(point, [p for p, c in zip(points.collect(), clusters.collect()) if c == cluster and not np.array_equal(p, point)])\\n        b = min(calculate_avg_distances(points.collect(), cluster, clusters.collect())) if len(set(clusters.collect())) > 1 else a\\n        s += (b - a) / max(a, b)\\n    return s / n\\n\\n# Вычисляем Silhouette Score для модели KMeans\\nscore = silhouette_score(rdd, clusters.predict(rdd))\\nprint(\"Silhouette Score: \", score)\\n\\n#'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'LAZY_LOAD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/magics/livy_executor.py\u001b[0m in \u001b[0;36mexecute_livy_statement\u001b[0;34m(self, url, session_name, variable_identifiers, return_variables, code, state_load_type)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mexecute_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_import_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecute_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecute_statement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/magics/livy_executor.py\u001b[0m in \u001b[0;36mexecute_statement\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mexecute_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvariables_import_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/magics/livy_executor.py\u001b[0m in \u001b[0;36m_create_statement\u001b[0;34m(self, url, session_id, code)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cancel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mstatement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/magics/livy_executor.py\u001b[0m in \u001b[0;36m_create_statement\u001b[0;34m(self, url, session_id, code)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"available\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cancelled\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cancel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-abq7rbdj --session gr0373-Vlasyuk-Serebryakova\n",
    "# ne rabotaet :|\n",
    "import numpy as np\n",
    "\n",
    "# Вычисляем расстояние между точками и центроидами кластеров\n",
    "def distance(point, center):\n",
    "    return np.linalg.norm(point - center)\n",
    "\n",
    "def calculate_distances(point, centers):\n",
    "    return [distance(point, center) for center in centers]\n",
    "\n",
    "# Вычисляем среднее расстояние до точек внутри кластера\n",
    "def avg_distance_in_cluster(point, cluster_points):\n",
    "    point = np.array(point)\n",
    "    cluster_points = [np.array(p) for p in cluster_points]\n",
    "    distances = [distance(point, p) for p in cluster_points if not np.array_equal(p, point)]\n",
    "    return sum(distances) / len(distances) if distances else 0.0\n",
    "\n",
    "def calculate_avg_distances(points, center, clusters):\n",
    "    points = [np.array(p) for p in points]\n",
    "    cluster_points = [p for p, c in zip(points, clusters) if c == center]\n",
    "    return [avg_distance_in_cluster(point, cluster_points) for point in cluster_points]\n",
    "\n",
    "# Вычисляем Silhouette Score\n",
    "def silhouette_score(points, clusters):\n",
    "    n = points.count()\n",
    "    s = 0.0\n",
    "    for point, cluster in zip(points.collect(), clusters.collect()):\n",
    "        a = avg_distance_in_cluster(point, [p for p, c in zip(points.collect(), clusters.collect()) if c == cluster and not np.array_equal(p, point)])\n",
    "        b = min(calculate_avg_distances(points.collect(), cluster, clusters.collect())) if len(set(clusters.collect())) > 1 else a\n",
    "        s += (b - a) / max(a, b)\n",
    "    return s / n\n",
    "\n",
    "# Вычисляем Silhouette Score для модели KMeans\n",
    "score = silhouette_score(rdd, clusters.predict(rdd))\n",
    "print(\"Silhouette Score: \", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "id": "f21b63e7",
   "metadata": {
    "cellId": "ujtnn42c50gzmz2do1wpk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cluster| count|\n",
      "+-------+------+\n",
      "|      0|671966|\n",
      "|      1|582659|\n",
      "|      2|666806|\n",
      "+-------+------+\n",
      "\n",
      "+-------+-----------+------+\n",
      "|cluster|activity_id| count|\n",
      "+-------+-----------+------+\n",
      "|      2|          1|  4520|\n",
      "|      0|          1|  8684|\n",
      "|      1|          1|179319|\n",
      "|      2|         12| 38145|\n",
      "|      1|         12| 10328|\n",
      "|      0|         12| 68743|\n",
      "|      2|         13| 40308|\n",
      "|      0|         13| 42245|\n",
      "|      1|         13| 22391|\n",
      "|      2|         16| 40299|\n",
      "|      0|         16|118166|\n",
      "|      1|         16| 16888|\n",
      "|      1|         17|107396|\n",
      "|      0|         17| 21406|\n",
      "|      2|         17|109888|\n",
      "|      1|          2|147484|\n",
      "|      0|          2|  5991|\n",
      "|      2|          2| 31713|\n",
      "|      0|         24|  4935|\n",
      "|      1|         24|  2819|\n",
      "|      2|         24| 28357|\n",
      "|      2|          3|107863|\n",
      "|      0|          3| 48418|\n",
      "|      1|          3| 33650|\n",
      "|      2|          4| 93981|\n",
      "|      1|          4| 10217|\n",
      "|      0|          4|133566|\n",
      "|      0|          5| 32921|\n",
      "|      1|          5| 12202|\n",
      "|      2|          5| 48916|\n",
      "|      1|          6| 11617|\n",
      "|      0|          6|112134|\n",
      "|      2|          6| 37814|\n",
      "|      2|          7| 85002|\n",
      "|      0|          7| 74757|\n",
      "|      1|          7| 28348|\n",
      "+-------+-----------+------+\n",
      "\n",
      "Execution time of the program is-  38.727219104766846\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "import time\n",
    "\n",
    "selected_columns = [\"hand_3D_acc_6_x\", \"hand_3D_acc_6_y\", \"hand_3D_acc_6_z\", \n",
    "                    \"chest_3D_acc_6_x\", \"chest_3D_acc_6_y\", \"chest_3D_acc_6_z\",\n",
    "                    \"ankle_3D_acc_6_x\", \"ankle_3D_acc_6_y\", \"ankle_3D_acc_6_z\",\n",
    "                    \"hand_3D_acc_16_x\", \"hand_3D_acc_16_y\", \"hand_3D_acc_16_z\",\n",
    "                    \"chest_3D_acc_16_x\", \"chest_3D_acc_16_y\", \"chest_3D_acc_16_z\",\n",
    "                    \"ankle_3D_acc_16_x\", \"ankle_3D_acc_16_y\", \"ankle_3D_acc_16_z\",\n",
    "                    \"hand_3D_gyro_x\", \"hand_3D_gyro_y\", \"hand_3D_gyro_z\",\n",
    "                    \"hand_3D_magnetometer_x\", \"hand_3D_magnetometer_y\", \"hand_3D_magnetometer_z\",\n",
    "                    \"hand_4D_orientation_w\",\n",
    "                    \"chest_3D_gyro_x\", \"chest_3D_gyro_y\", \"chest_3D_gyro_z\",\n",
    "                    \"chest_3D_magnetometer_x\", \"chest_3D_magnetometer_y\", \"chest_3D_magnetometer_z\",\n",
    "                    \"chest_4D_orientation_w\",\n",
    "                    \"ankle_3D_gyro_x\", \"ankle_3D_gyro_y\", \"ankle_3D_gyro_z\",\n",
    "                    \"ankle_3D_magnetometer_x\", \"ankle_3D_magnetometer_y\", \"ankle_3D_magnetometer_z\",\n",
    "                    \"ankle_4D_orientation_w\", \n",
    "                   ]\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# for column in selected_columns:\n",
    "#     pm_select = pamap_data.select(selected_columns).withColumn(column, col(column).cast(\"double\"))\n",
    "pamap_data_selected = pamap_data.select(\n",
    "    [col(c).cast(\"double\").alias(c) for c in selected_columns]\n",
    ")\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "pm_select = vecAssembler.transform(pamap_data_selected)\n",
    "\n",
    "rdd = pm_select.select(\"features\").rdd.map(lambda x: Vectors.dense(x[0]))\n",
    "\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(rdd)\n",
    "normalizedData = scalerModel.transform(rdd)\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "#starting time\n",
    "start = time.time()\n",
    "clusters = KMeans.train(rdd, 3, maxIterations=10, initializationMode=\"random\")\n",
    "end = time.time()\n",
    "\n",
    "predictions = rdd.map(lambda x: clusters.predict(x))\n",
    "\n",
    "\n",
    "\n",
    "pred_df = predictions.map(lambda x: (x, )).toDF([\"cluster\"])\n",
    "pred_df = pred_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Добавляем индекс к исходному DataFrame и объединяем по индексу\n",
    "pamap_data = pamap_data.withColumn(\"id\", monotonically_increasing_id())\n",
    "result_df = pamap_data.join(pred_df, on=\"id\", how=\"inner\").drop(\"id\")\n",
    "\n",
    "# вывод количества записей в каждом кластере\n",
    "result_df.groupBy(\"cluster\").count().show()\n",
    "\n",
    "# вывод типов активности в каждом кластере\n",
    "result_df.groupBy(\"cluster\", \"activity_id\").count().orderBy([\"activity_id\"]).show(50)\n",
    "\n",
    "print(\"Execution time of the program is- \", end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "id": "b556ada7",
   "metadata": {
    "cellId": "sk48zpcz7c8ewqzynv6gma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 103128882.05271979\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = rdd.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1424,
   "id": "68b9b1e1",
   "metadata": {
    "cellId": "1jgchj0m4bnj0gzh2frvv"
   },
   "outputs": [
    {
     "ename": "Execute error",
     "evalue": "Chosen cluster was not found.\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-abq7rbdj --session gr0373-Vlasyuk-Serebryakova\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def train_gmm(subject_rdd):\n",
    "    # Создание экземпляра модели GMM\n",
    "    gmm = GaussianMixture.train(subject_rdd, k=3, maxIterations=10)\n",
    "\n",
    "    # Предсказание кластера для каждой точки в RDD\n",
    "    predictions = subject_rdd.map(lambda x: gmm.predict(x))\n",
    "    return predictions\n",
    "\n",
    "# объединяем все rdd по кластерам\n",
    "rdd = subject_rdd_list[0]\n",
    "for subject_rdd in subject_rdd_list[1:]:\n",
    "    rdd = rdd.union(subject_rdd)\n",
    "\n",
    "# train GMM\n",
    "predictions = train_gmm(rdd)\n",
    "\n",
    "# Преобразование RDD в DataFrame\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "pred_df = predictions.map(lambda x: (x, )).toDF([\"cluster\"])\n",
    "pred_df = pred_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Добавляем индекс к исходному DataFrame и объединяем по индексу\n",
    "pamap_data = pamap_data.withColumn(\"id\", monotonically_increasing_id())\n",
    "result_df = pamap_data.join(pred_df, on=\"id\", how=\"inner\").drop(\"id\")\n",
    "\n",
    "# вывод количества записей в каждом кластере\n",
    "result_df.groupBy(\"cluster\").count().show()\n",
    "\n",
    "# вывод типов активности в каждом кластере\n",
    "result_df.groupBy(\"cluster\", \"activity_id\").count().orderBy([\"activity_id\"]).show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7c96b",
   "metadata": {
    "cellId": "8wvolssjtsili2bqodle3",
    "execution_id": "bb509a08-47f6-4da9-9e2e-26df569efb54"
   },
   "source": [
    "# mirustal \n",
    "\n",
    "проба в классификацию потому что с кластеризацией трэээш"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "id": "6cf41dd9",
   "metadata": {
    "cellId": "nzza2jvsmani8cvy6iis"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "def load_activity_map():\n",
    "    map = {}\n",
    "    map[0] = 'transient'\n",
    "    map[1] = 'lying'\n",
    "    map[2] = 'sitting'\n",
    "    map[3] = 'standing'\n",
    "    map[4] = 'walking'\n",
    "    map[5] = 'running'\n",
    "    map[6] = 'cycling'\n",
    "    map[7] = 'Nordic_walking'\n",
    "    map[9] = 'watching_TV'\n",
    "    map[10] = 'computer_work'\n",
    "    map[11] = 'car driving'\n",
    "    map[12] = 'ascending_stairs'\n",
    "    map[13] = 'descending_stairs'\n",
    "    map[16] = 'vacuum_cleaning'\n",
    "    map[17] = 'ironing'\n",
    "    map[18] = 'folding_laundry'\n",
    "    map[19] = 'house_cleaning'\n",
    "    map[20] = 'playing_soccer'\n",
    "    map[24] = 'rope_jumping'\n",
    "    return map\n",
    "\n",
    "def generate_three_IMU(name):\n",
    "    x = name +'_x'\n",
    "    y = name +'_y'\n",
    "    z = name +'_z'\n",
    "    return [x,y,z]\n",
    "\n",
    "def generate_four_IMU(name):\n",
    "    x = name +'_x'\n",
    "    y = name +'_y'\n",
    "    z = name +'_z'\n",
    "    w = name +'_w'\n",
    "    return [x,y,z,w]\n",
    "\n",
    "def generate_cols_IMU(name):\n",
    "    # temp\n",
    "    temp = name+'_temperature'\n",
    "    output = [temp]\n",
    "    # acceleration 16\n",
    "    acceleration16 = name+'_3D_acc_16'\n",
    "    acceleration16 = generate_three_IMU(acceleration16)\n",
    "    output.extend(acceleration16)\n",
    "    # acceleration 6\n",
    "    acceleration6 = name+'_3D_acc_6'\n",
    "    acceleration6 = generate_three_IMU(acceleration6)\n",
    "    output.extend(acceleration6)\n",
    "    # gyroscope\n",
    "    gyroscope = name+'_3D_gyro'\n",
    "    gyroscope = generate_three_IMU(gyroscope)\n",
    "    output.extend(gyroscope)\n",
    "    # magnometer\n",
    "    magnometer = name+'_3D_magnetometer'\n",
    "    magnometer = generate_three_IMU(magnometer)\n",
    "    output.extend(magnometer)\n",
    "    # oreintation\n",
    "    oreintation = name+'_4D_orientation'\n",
    "    oreintation = generate_four_IMU(oreintation)\n",
    "    output.extend(oreintation)\n",
    "    return output\n",
    "\n",
    "def load_IMU():\n",
    "    output = ['time_stamp','activity_id', 'heart_rate']\n",
    "    hand = 'hand'\n",
    "    hand = generate_cols_IMU(hand)\n",
    "    output.extend(hand)\n",
    "    chest = 'chest'\n",
    "    chest = generate_cols_IMU(chest)\n",
    "    output.extend(chest)\n",
    "    ankle = 'ankle'\n",
    "    ankle = generate_cols_IMU(ankle)\n",
    "    output.extend(ankle)\n",
    "    return output\n",
    "\n",
    "def load_IMU_noTS():\n",
    "    output = ['activity_id', 'heart_rate']\n",
    "    hand = 'hand'\n",
    "    hand = generate_cols_IMU(hand)\n",
    "    output.extend(hand)\n",
    "    chest = 'chest'\n",
    "    chest = generate_cols_IMU(chest)\n",
    "    output.extend(chest)\n",
    "    ankle = 'ankle'\n",
    "    ankle = generate_cols_IMU(ankle)\n",
    "    output.extend(ankle)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def load_subjects(root=r\"/user/bigdata-course/PAMAP2 Physical Activity Monitoring/PAMAP2_Dataset/Protocol/\"):\n",
    "    cols = load_IMU()\n",
    "    \n",
    "    path = root + \"*.dat\"\n",
    "    rdd = sc.textFile(path)\n",
    "    numbers_rdd = rdd.map(lambda x : (x.split(\" \"))).filter(lambda x: x[1] != '0')\n",
    "    output = numbers_rdd.toDF(cols)\n",
    "    return output\n",
    "\n",
    "cluster_data = load_subjects()\n",
    "\n",
    "\n",
    "for i in range(1, 10):\n",
    "    #rdd = sc.textFile(path + f\"subject{i}.dat\")\n",
    "    exec(f'subject_{i} = sc.textFile(root + f\"subject10{i}.dat\").map(lambda x : (x.split(\" \"))).filter(lambda x: x[1] != \"0\")')\n",
    "\n",
    "\n",
    "index = []\n",
    "values = []\n",
    "for i in range(1, 10):\n",
    "    index.append(f\"subject_{i}\")\n",
    "    values.append(globals()[index[-1]].count())\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sum_line(line):  \n",
    "#For eache line \n",
    "    for i in range(number_of_feature):\n",
    "        if(line[i]!='NaN'):\n",
    "            sum_of_a_feature[i].add(float(line[i]))\n",
    "\n",
    "def avg_column(accumulator,number_of_row):\n",
    "#Get the average\n",
    "    avg_feature=[]\n",
    "    count=0\n",
    "    for element in sum_of_a_feature:\n",
    "        avg=element.value/number_of_row\n",
    "        value=avg\n",
    "        avg_feature.append(value)\n",
    "        count+=1\n",
    "    return avg_feature\n",
    "\n",
    "def replace_missing_value(line,avg_feature):\n",
    "    for i in range(number_of_feature):\n",
    "        if(line[i]=='NaN'):\n",
    "            line[i]=float(avg_feature[i])\n",
    "        else:\n",
    "            line[i]=float(line[i])\n",
    "    return line\n",
    "\n",
    "def parsePoint(values):\n",
    "    values = list(map(float, values))\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "  \n",
    "def process_subject(subject,number_of_feature,sum_of_a_feature):\n",
    "    number_of_row=subject.count()\n",
    "#     print(number_of_row)\n",
    "#Compute sum for each column\n",
    "    subject.foreach(sum_line)\n",
    "#     print(subject.count())\n",
    "#Compute avg of each column\n",
    "    avg_feature=avg_column(sum_of_a_feature,number_of_row)\n",
    "#     print(avg_feature)\n",
    "#Replace missing value\n",
    "    subject_without_missing_value=subject.map(lambda j: replace_missing_value(j,avg_feature))\n",
    "#     print(subject_without_missing_value.count())\n",
    "#     print(subject_without_missing_value)\n",
    "#lambda j: processDataLine(j, arg1, arg2)\n",
    "    return subject_without_missing_value\n",
    "\n",
    "def insert_label(x):\n",
    "    result=x[1].tolist()\n",
    "    result.insert(0,x[0])\n",
    "    return result\n",
    "\n",
    "number_of_row=subject_8.count()\n",
    "number_of_feature=54\n",
    "sum_of_a_feature = [sc.accumulator(0) for x in range(number_of_feature)]\n",
    "#print(subject_101.take(5))\n",
    "\n",
    "\n",
    "subject_rdd_list = []\n",
    "for i in range(1, 9):\n",
    "    #rdd = sc.textFile(path + f\"subject{i}.dat\")\n",
    "    exec(f'subject{i} = process_subject(subject_{i},number_of_feature,sum_of_a_feature).map(lambda x:(((x[1],int(x[0]/5)),(np.asarray(x[2:]),1)))).reduceByKey(lambda a,b: (np.sum([a[0],b[0]], axis=0),a[1]+b[1])).map(lambda x: ((x[0][0]),np.true_divide(x[1][0],x[1][1]))).map(insert_label)')\n",
    "    #exec(f'subject{i} = process_subject(subject_{i},number_of_feature,sum_of_a_feature)')\n",
    "    \n",
    "    subject_rdd_list.append(globals()[f'subject{i}'])\n",
    "\n",
    "clus_rdd = sc.union(subject_rdd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "id": "e70ada3e",
   "metadata": {
    "cellId": "41k2te53w82onlm9s7f1y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53, PythonRDD[331] at RDD at PythonRDD.scala:53, PythonRDD[332] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53, PythonRDD[331] at RDD at PythonRDD.scala:53, PythonRDD[332] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53, PythonRDD[331] at RDD at PythonRDD.scala:53, PythonRDD[332] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53, PythonRDD[331] at RDD at PythonRDD.scala:53, PythonRDD[332] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53, PythonRDD[331] at RDD at PythonRDD.scala:53, PythonRDD[332] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[331] at RDD at PythonRDD.scala:53, PythonRDD[332] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53, PythonRDD[332] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53, PythonRDD[331] at RDD at PythonRDD.scala:53]\n",
      "Time Random Forest  315.100444316864\n",
      "Average Error Random Forest \n",
      "0.27713947103382597\n",
      "Time LogisticRegression  117.90742087364197\n",
      "Average Error Logistic Regression\n",
      "0.27713947103382597\n",
      "Time DecisionTree  10.582316875457764\n",
      "Average Error Decision Tree\n",
      "0.35115654129862717\n",
      "Time Naive Bayes  7.370604038238525\n",
      "Average Error Naive Bayes\n",
      "0.3610948413948317\n",
      "RF: \n",
      "Доля верных предсказаний: 81.62650602409639%\n",
      "Time Random Forest  39.60712242126465\n",
      "Precision = 0.8568109854950162\n",
      "Recall = 0.8162650602409638\n",
      "F1-score = 0.8234861783741904\n",
      "accuracy: = 0.8162650602409639\n",
      "Precision = 0.8568109854950162\n",
      "Recall = 0.8162650602409638\n",
      "F1-score = 0.8234861783741904\n",
      "accuracy: = 0.8162650602409639\n",
      "LR:\n",
      "Доля верных предсказаний: 89.375%\n",
      "Time LogisticRegression  13.21681022644043\n",
      "Precision = 0.8963586328363449\n",
      "Recall = 0.8937499999999999\n",
      "F1-score = 0.8938959225602935\n",
      "accuracy: = 0.89375\n",
      "DT:\n",
      "Доля верных предсказаний: 75.31734837799718%\n",
      "Time LogisticRegression  1.1310973167419434\n",
      "Precision = 0.819245261673875\n",
      "Recall = 0.7531734837799718\n",
      "F1-score = 0.7489709645979782\n",
      "accuracy: = 0.7531734837799718\n",
      "NB:\n",
      "Доля верных предсказаний: 70.41602465331279%\n",
      "Time LogisticRegression  0.720489501953125\n",
      "Precision = 0.7157466426947218\n",
      "Recall = 0.7041602465331278\n",
      "F1-score = 0.703079492751453\n",
      "accuracy: = 0.7041602465331279\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "\n",
    "\n",
    "list_error_rf=[]\n",
    "list_error_lr=[]\n",
    "list_error_dt=[]\n",
    "\n",
    "time_rf = 0\n",
    "time_dt = 0\n",
    "time_nb = 0\n",
    "time_lr = 0\n",
    "for i in range(len(subject_rdd_list)):\n",
    "    test_subject=subject_rdd_list[i]\n",
    "    # Cоздаем новый список, который не включает в себя i-ый элемент\n",
    "    training_subjects = subject_rdd_list[:i] + subject_rdd_list[i+1:]\n",
    "    #Объединяем все RDD-объекты, кроме i-ого, в один RDD-объект\n",
    "    dataset_subject_without_missing_value=sc.union(training_subjects)\n",
    "    print(training_subjects)\n",
    "    # Загружаем и парсим файл данных в RDD-объекты типа LabeledPoint\n",
    "\n",
    "    data = dataset_subject_without_missing_value.map(parsePoint)\n",
    "    # Разделяем данные на обучающую и тестовую выборки (30% данных оставляем на тестирование)\n",
    "    trainingData = data # Все объекты, кроме i-го, используем для обучения\n",
    "    testData = test_subject.map(parsePoint)  # Используем i-й объект для тестирования\n",
    "\n",
    "    ################################################################# MODEL #############################################################################\n",
    "    ###########Train a RandomForest model.######################\n",
    "    '''\n",
    "    Случайный лес (Random Forest) - это алгоритм машинного обучения, который используется\n",
    "    для задач классификации, регрессии и кластеризации.\n",
    "    Он состоит из множества решающих деревьев, которые обучаются на подвыборках данных\n",
    "    и подмножествах признаков. Во время обучения каждое дерево строится независимо\n",
    "    от других деревьев, и каждое дерево получает случайный поднабор данных. После обучения,\n",
    "    классификация новых данных происходит путем применения каждого дерева к новым данным и выбора\n",
    "    класса с наибольшим количеством голосов.\n",
    "    \n",
    "    \n",
    "    \n",
    "    numClasses: количество классов, на которые нужно разделить данные.\n",
    "    categoricalFeaturesInfo: информация о категориальных признаках. Пустое значение означает, что все признаки являются непрерывными.\n",
    "    numTrees: количество деревьев в лесу. В данном случае равно 2000.\n",
    "    featureSubsetStrategy: стратегия выбора подмножества признаков для каждого дерева. Установлено значение \"auto\", которое позволяет алгоритму выбрать подходящую стратегию.\n",
    "    impurity: критерий для оценки качества разделения узлов в деревьях. Установлено значение 'gini'.\n",
    "    maxDepth: максимальная глубина деревьев. Установлено значение 5.\n",
    "    maxBins: максимальное количество корзин для дискретизации признаков. Установлено значение 32.\n",
    "    '''\n",
    "    timeRF_start = time.time()\n",
    "    model = RandomForest.trainClassifier(trainingData, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                       numTrees=2000, featureSubsetStrategy=\"auto\",\n",
    "                                       impurity='gini', maxDepth=5, maxBins=32)\n",
    "    timeRF_end = time.time()\n",
    "    time_rf += timeRF_end - timeRF_start\n",
    "    # Оценка модели на тестовых данных и вычисление ошибки тестирования\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    \n",
    "    testErr = labelsAndPredictions.filter(lambda vp: vp[0] != vp[1]).count()  / float(testData.count())\n",
    "    list_error_rf.append(testErr) \n",
    "    #Save the error on the leave one out \n",
    "    '''\n",
    "    \"Leave one out\" (LOO) - это метод кросс-валидации, используемый для оценки производительности\n",
    "    моделей машинного обучения. Он заключается в том, что одно наблюдение удаляется из обучающего\n",
    "    набора данных, модель обучается на оставшихся данных и затем используется для прогнозирования\n",
    "    значения удаленного наблюдения. Этот процесс повторяется для всех наблюдений в обучающем наборе,\n",
    "    и каждый раз ошибка прогнозирования рассчитывается для удаленного наблюдения.\n",
    "    list_error_rf.append(testErr) \n",
    "    '''\n",
    "    #print('Test Error = ' + str(testErr))\n",
    "    ########### Logistic Regression model.#######\n",
    "    '''\n",
    "    Logistic regression (логистическая регрессия) - это метод машинного обучения, который применяется\n",
    "    для задач классификации. Он использует логистическую функцию, чтобы преобразовать\n",
    "    линейную комбинацию входных признаков в вероятность отнесения объекта к определенному классу.\n",
    "\n",
    "    В данном коде происходит обучение модели логистической регрессии с помощью метода оптимизации LBFGS\n",
    "    (Limited-memory Broyden–Fletcher–Goldfarb–Shanno).\n",
    "    Этот метод используется для решения задач оптимизации без ограничений с помощью градиентного спуска.\n",
    "\n",
    "    Параметры:\n",
    "    trainingData - обучающая выборка;\n",
    "    numClasses - число классов в задаче классификации;\n",
    "    '''\n",
    "    timeLR_start = time.time()\n",
    "    model_regression = LogisticRegressionWithLBFGS.train(trainingData,numClasses=25)\n",
    "    timeLR_end = time.time()\n",
    "    time_lr += timeLR_end - timeLR_start\n",
    "    # Оценка модели на тестовых данных\n",
    "    labelsAndPreds = testData.map(lambda p: (p.label, model_regression.predict(p.features)))\n",
    "    trainErr = labelsAndPreds.filter(lambda vp: vp[0] != vp[1]).count() / float(data.count())\n",
    "    # Сохранение ошибки на leave one out\n",
    "    list_error_lr.append(testErr)\n",
    "    #print(\"Training Error = \" + str(trainErr))\n",
    "    \n",
    "    ########### Decision Tree .######################\n",
    "    '''\n",
    "    Decision Tree (дерево решений) - это один из самых популярных алгоритмов машинного обучения,\n",
    "    который используется как для задач классификации, так и для задач регрессии.\n",
    "    Он представляет собой структуру дерева, где каждый узел представляет собой тест на определенный\n",
    "    признак, каждое ребро соответствует значению этого признака,\n",
    "    а каждый лист соответствует конкретному классу или значению целевой переменной.\n",
    "    \n",
    "    Параметры:\n",
    "\n",
    "    numClasses: число уникальных классов в целевой переменной.\n",
    "    categoricalFeaturesInfo: словарь, который содержит информацию о категориальных признаках. По умолчанию все признаки считаются непрерывными.\n",
    "    impurity: функция, которая используется для вычисления неопределенности узла. Возможные значения - \"gini\" для индекса Джини и \"entropy\" для энтропии.\n",
    "    maxDepth: максимальная глубина дерева. Более глубокое дерево может привести к переобучению, тогда как более неглубокое дерево может привести к недообучению.\n",
    "    maxBins: максимальное количество корзин, используемых для дискретизации непрерывных признаков.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Пустые categoricalFeaturesInfo указывают на то, что все признаки являются непрерывными.\n",
    "    timeDT_start = time.time()\n",
    "    model = DecisionTree.trainClassifier(trainingData, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "    timeDT_end = time.time()\n",
    "    time_dt += timeDT_end - timeDT_start\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    testErr = labelsAndPredictions.filter(lambda vp: vp[0] != vp[1]).count() / float(testData.count())\n",
    "   # Оценка модели на тестовых данных и вычисление ошибки\n",
    "    list_error_dt.append(testErr)\n",
    "    #print('Test Error = ' + str(testErr))\n",
    "    #print(model.toDebugString())\n",
    "    \n",
    "    \n",
    "########### Naives Bayes .######################\n",
    "# Список RDD, содержащих данные по различным предметам\n",
    "subject_rdd_list=[subject1,subject2,subject3,subject4,subject5,subject6, subject7, subject8]\n",
    "list_error_nb=[]\n",
    "# timeNB_start = []\n",
    "# timeNB_end = []\n",
    "for i in range(len(subject_rdd_list)):\n",
    "    subject_rdd_list=[subject1,subject2,subject3,subject4,subject5,subject6, subject7, subject8]\n",
    "    # Извлечение тестовых данных (одного предмета)\n",
    "    test_subject=subject_rdd_list[i].map(lambda x: (map(abs, x)))\n",
    "    del subject_rdd_list[i] # Исключение тестовых данных из списка RDD\n",
    "    #Объединение оставшихся RDD, чтобы сформировать обучающий набор данных\n",
    "    dataset_subject_without_missing_value=sc.union(subject_rdd_list)\n",
    "    # Загрузка и парсинг файлов с данными в RDD объект LabeledPoint\n",
    "    data = dataset_subject_without_missing_value.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "    trainingData = data \n",
    "    testData = test_subject.map(parsePoint)\n",
    "    ########## Naive Bayes model.######################\n",
    "    '''\n",
    "    Наивный Байесовский классификатор - это вероятностный алгоритм машинного обучения,\n",
    "    основанный на теореме Байеса. Он используется для классификации объектов на основе комбинации\n",
    "    априорной вероятности и правдоподобия условия.\n",
    "\n",
    "    В машинном обучении наивный Байесовский классификатор является простым и эффективным алгоритмом,\n",
    "    особенно для категориальных и текстовых данных. Он основывается на предположении о независимости\n",
    "    между признаками, то есть каждый признак вносит независимый вклад в прогноз классификации объекта.\n",
    "\n",
    "    lambda_: Параметр сглаживания Лапласа, который используется для предотвращения вероятности,\n",
    "    равной нулю.\n",
    "    '''\n",
    "    timeNB_start = time.time()\n",
    "    model = NaiveBayes.train(trainingData, 1.0)\n",
    "    timeNB_end = time.time()\n",
    "    time_nb += timeNB_end - timeNB_start\n",
    "     # Прогнозирование и проверка точности\n",
    "    predictionAndLabel = testData.map(lambda p: (model.predict(p.features), p.label))\n",
    "    accuracy = 1.0 * predictionAndLabel.filter(lambda vp: vp[0] != vp[1]).count() / testData.count()\n",
    "    list_error_nb.append(accuracy)\n",
    "\n",
    "print(\"Time Random Forest \", time_rf)\n",
    "print(\"Average Error Random Forest \")\n",
    "print(reduce(lambda x, y: x + y, list_error_rf) / len(list_error_rf))\n",
    "\n",
    "\n",
    "print(\"Time LogisticRegression \", time_lr)\n",
    "print(\"Average Error Logistic Regression\")\n",
    "print(reduce(lambda x, y: x + y, list_error_lr) / len(list_error_lr))\n",
    "\n",
    "\n",
    "print(\"Time DecisionTree \", time_dt)\n",
    "print(\"Average Error Decision Tree\")\n",
    "print(reduce(lambda x, y: x + y, list_error_dt) / len(list_error_dt))\n",
    "\n",
    "\n",
    "print(\"Time Naive Bayes \", time_nb)\n",
    "print(\"Average Error Naive Bayes\")\n",
    "print(reduce(lambda x, y: x + y, list_error_nb) / len(list_error_nb))\n",
    "\n",
    "\n",
    "print(\"RF: \")\n",
    "#####################################      RF\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint(row[0], row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint(row[0], row[1:]))\n",
    "time_s = time.time()\n",
    "model = RandomForest.trainClassifier(train_rdd, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                       numTrees=2000, featureSubsetStrategy=\"auto\",\n",
    "                                       impurity='gini', maxDepth=5, maxBins=32)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time Random Forest \",  time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "print(\"LR:\")\n",
    "####################################  LR\n",
    "\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = LogisticRegressionWithLBFGS.train(train_rdd,numClasses=25)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "labels_and_predictions = test_rdd.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "\n",
    "\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "\n",
    "print(\"DT:\")\n",
    "########################################################### DT\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = DecisionTree.trainClassifier(train_rdd, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "print(\"NB:\")\n",
    "####################################### NB\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "test_rdd = test_df.rdd.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = NaiveBayes.train(train_rdd, 1.0)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "labels_and_predictions = test_rdd.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "id": "1c8d2ca4",
   "metadata": {
    "cellId": "byffs56hrzlrnr22xjwkfj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[330] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53, PythonRDD[328] at RDD at PythonRDD.scala:53, PythonRDD[329] at RDD at PythonRDD.scala:53]\n",
      "Time Random Forest  185.0582778453827\n",
      "Average Error Random Forest \n",
      "0.28691413027076607\n",
      "Time LogisticRegression  63.09696412086487\n",
      "Average Error Logistic Regression\n",
      "0.28691413027076607\n",
      "Time DecisionTree  4.61385703086853\n",
      "Average Error Decision Tree\n",
      "0.4533238878222238\n",
      "Time Naive Bayes  4.702111005783081\n",
      "Average Error Naive Bayes\n",
      "0.3610948413948317\n",
      "RF: \n",
      "Доля верных предсказаний: 80.18154311649016%\n",
      "Time Random Forest  42.197484493255615\n",
      "Precision = 0.8513315290234623\n",
      "Recall = 0.8018154311649016\n",
      "F1-score = 0.8101375933617977\n",
      "accuracy: = 0.8018154311649016\n",
      "Precision = 0.8513315290234623\n",
      "Recall = 0.8018154311649016\n",
      "F1-score = 0.8101375933617977\n",
      "accuracy: = 0.8018154311649016\n",
      "LR:\n",
      "Доля верных предсказаний: 90.70735090152566%\n",
      "Time LogisticRegression  12.552348852157593\n",
      "Precision = 0.9087282771094339\n",
      "Recall = 0.9070735090152564\n",
      "F1-score = 0.9069450232869984\n",
      "accuracy: = 0.9070735090152566\n",
      "DT:\n",
      "Доля верных предсказаний: 75.31206657420249%\n",
      "Time LogisticRegression  2.3628852367401123\n",
      "Precision = 0.8354403997144331\n",
      "Recall = 0.7531206657420249\n",
      "F1-score = 0.7652134495350322\n",
      "accuracy: = 0.753120665742025\n",
      "NB:\n",
      "Доля верных предсказаний: 67.90490341753343%\n",
      "Time LogisticRegression  1.5566630363464355\n",
      "Precision = 0.6860273316039044\n",
      "Recall = 0.6790490341753344\n",
      "F1-score = 0.6771140318379193\n",
      "accuracy: = 0.6790490341753344\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "\n",
    "\n",
    "list_error_rf=[]\n",
    "list_error_lr=[]\n",
    "list_error_dt=[]\n",
    "subject_rdd_list = subject_rdd_list[0:6]\n",
    "\n",
    "time_rf = 0\n",
    "time_dt = 0\n",
    "time_nb = 0\n",
    "time_lr = 0\n",
    "for i in range(len(subject_rdd_list)):\n",
    "    test_subject=subject_rdd_list[i]\n",
    "    # Cоздаем новый список, который не включает в себя i-ый элемент\n",
    "    training_subjects = subject_rdd_list[:i] + subject_rdd_list[i+1:]\n",
    "    #Объединяем все RDD-объекты, кроме i-ого, в один RDD-объект\n",
    "    dataset_subject_without_missing_value=sc.union(training_subjects)\n",
    "    print(training_subjects)\n",
    "    # Загружаем и парсим файл данных в RDD-объекты типа LabeledPoint\n",
    "\n",
    "    data = dataset_subject_without_missing_value.map(parsePoint)\n",
    "    # Разделяем данные на обучающую и тестовую выборки (30% данных оставляем на тестирование)\n",
    "    trainingData = data # Все объекты, кроме i-го, используем для обучения\n",
    "    testData = test_subject.map(parsePoint)  # Используем i-й объект для тестирования\n",
    "\n",
    "    ################################################################# MODEL #############################################################################\n",
    "    ###########Train a RandomForest model.######################\n",
    "    '''\n",
    "    Случайный лес (Random Forest) - это алгоритм машинного обучения, который используется\n",
    "    для задач классификации, регрессии и кластеризации.\n",
    "    Он состоит из множества решающих деревьев, которые обучаются на подвыборках данных\n",
    "    и подмножествах признаков. Во время обучения каждое дерево строится независимо\n",
    "    от других деревьев, и каждое дерево получает случайный поднабор данных. После обучения,\n",
    "    классификация новых данных происходит путем применения каждого дерева к новым данным и выбора\n",
    "    класса с наибольшим количеством голосов.\n",
    "    \n",
    "    \n",
    "    \n",
    "    numClasses: количество классов, на которые нужно разделить данные.\n",
    "    categoricalFeaturesInfo: информация о категориальных признаках. Пустое значение означает, что все признаки являются непрерывными.\n",
    "    numTrees: количество деревьев в лесу. В данном случае равно 2000.\n",
    "    featureSubsetStrategy: стратегия выбора подмножества признаков для каждого дерева. Установлено значение \"auto\", которое позволяет алгоритму выбрать подходящую стратегию.\n",
    "    impurity: критерий для оценки качества разделения узлов в деревьях. Установлено значение 'gini'.\n",
    "    maxDepth: максимальная глубина деревьев. Установлено значение 5.\n",
    "    maxBins: максимальное количество корзин для дискретизации признаков. Установлено значение 32.\n",
    "    '''\n",
    "    timeRF_start = time.time()\n",
    "    model = RandomForest.trainClassifier(trainingData, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                       numTrees=2000, featureSubsetStrategy=\"auto\",\n",
    "                                       impurity='gini', maxDepth=5, maxBins=32)\n",
    "    timeRF_end = time.time()\n",
    "    time_rf += timeRF_end - timeRF_start\n",
    "    # Оценка модели на тестовых данных и вычисление ошибки тестирования\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    \n",
    "    testErr = labelsAndPredictions.filter(lambda vp: vp[0] != vp[1]).count()  / float(testData.count())\n",
    "    list_error_rf.append(testErr) \n",
    "    #Save the error on the leave one out \n",
    "    '''\n",
    "    \"Leave one out\" (LOO) - это метод кросс-валидации, используемый для оценки производительности\n",
    "    моделей машинного обучения. Он заключается в том, что одно наблюдение удаляется из обучающего\n",
    "    набора данных, модель обучается на оставшихся данных и затем используется для прогнозирования\n",
    "    значения удаленного наблюдения. Этот процесс повторяется для всех наблюдений в обучающем наборе,\n",
    "    и каждый раз ошибка прогнозирования рассчитывается для удаленного наблюдения.\n",
    "    list_error_rf.append(testErr) \n",
    "    '''\n",
    "    #print('Test Error = ' + str(testErr))\n",
    "    ########### Logistic Regression model.#######\n",
    "    '''\n",
    "    Logistic regression (логистическая регрессия) - это метод машинного обучения, который применяется\n",
    "    для задач классификации. Он использует логистическую функцию, чтобы преобразовать\n",
    "    линейную комбинацию входных признакоов в вероятность отнесения объекта к определенному классу.\n",
    "\n",
    "    В данном коде происходит обучение модели логистической регрессии с помощью метода оптимизации LBFGS\n",
    "    (Limited-memory Broyden–Fletcher–Goldfarb–Shanno).\n",
    "    Этот метод используется для решения задач оптимизации без ограничений с помощью градиентного спуска.\n",
    "\n",
    "    Параметры:\n",
    "    trainingData - обучающая выборка;\n",
    "    numClasses - число классов в задаче классификации;\n",
    "    '''\n",
    "    timeLR_start = time.time()\n",
    "    model_regression = LogisticRegressionWithLBFGS.train(trainingData,numClasses=25)\n",
    "    timeLR_end = time.time()\n",
    "    time_lr += timeLR_end - timeLR_start\n",
    "    # Оценка модели на тестовых данных\n",
    "    labelsAndPreds = testData.map(lambda p: (p.label, model_regression.predict(p.features)))\n",
    "    trainErr = labelsAndPreds.filter(lambda vp: vp[0] != vp[1]).count() / float(data.count())\n",
    "    # Сохранение ошибки на leave one out\n",
    "    list_error_lr.append(testErr)\n",
    "    #print(\"Training Error = \" + str(trainErr))\n",
    "    \n",
    "    ########### Decision Tree .######################\n",
    "    '''\n",
    "    Decision Tree (дерево решений) - это один из самых популярных алгоритмов машинного обучения,\n",
    "    который используется как для задач классификации, так и для задач регрессии.\n",
    "    Он представляет собой структуру дерева, где каждый узел представляет собой тест на определенный\n",
    "    признак, каждое ребро соответствует значению этого признака,\n",
    "    а каждый лист соответствует конкретному классу или значению целевой переменной.\n",
    "    \n",
    "    Параметры:\n",
    "\n",
    "    numClasses: число уникальных классов в целевой переменной.\n",
    "    categoricalFeaturesInfo: словарь, который содержит информацию о категориальных признаках. По умолчанию все признаки считаются непрерывными.\n",
    "    impurity: функция, которая используется для вычисления неопределенности узла. Возможные значения - \"gini\" для индекса Джини и \"entropy\" для энтропии.\n",
    "    maxDepth: максимальная глубина дерева. Более глубокое дерево может привести к переобучению, тогда как более неглубокое дерево может привести к недообучению.\n",
    "    maxBins: максимальное количество корзин, используемых для дискретизации непрерывных признаков.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Пустые categoricalFeaturesInfo указывают на то, что все признаки являются непрерывными.\n",
    "    timeDT_start = time.time()\n",
    "    model = DecisionTree.trainClassifier(trainingData, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "    timeDT_end = time.time()\n",
    "    time_dt += timeDT_end - timeDT_start\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    testErr = labelsAndPredictions.filter(lambda vp: vp[0] != vp[1]).count() / float(testData.count())\n",
    "   # Оценка модели на тестовых данных и вычисление ошибки\n",
    "    list_error_dt.append(testErr)\n",
    "    #print('Test Error = ' + str(testErr))\n",
    "    #print(model.toDebugString())\n",
    "    \n",
    "    \n",
    "########### Naives Bayes .######################\n",
    "# Список RDD, содержащих данные по различным предметам\n",
    "subject_rdd_list=[subject1,subject2,subject3,subject4,subject5,subject6, subject7, subject8]\n",
    "list_error_nb=[]\n",
    "# timeNB_start = []\n",
    "# timeNB_end = []\n",
    "for i in range(len(subject_rdd_list)):\n",
    "    subject_rdd_list=[subject1,subject2,subject3,subject4,subject5,subject6, subject7, subject8]\n",
    "    # Извлечение тестовых данных (одного предмета)\n",
    "    test_subject=subject_rdd_list[i].map(lambda x: (map(abs, x)))\n",
    "    del subject_rdd_list[i] # Исключение тестовых данных из списка RDD\n",
    "    #Объединение оставшихся RDD, чтобы сформировать обучающий набор данных\n",
    "    dataset_subject_without_missing_value=sc.union(subject_rdd_list)\n",
    "    # Загрузка и парсинг файлов с данными в RDD объект LabeledPoint\n",
    "    data = dataset_subject_without_missing_value.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "    trainingData = data \n",
    "    testData = test_subject.map(parsePoint)\n",
    "    ########## Naive Bayes model.######################\n",
    "    '''\n",
    "    Наивный Байесовский классификатор - это вероятностный алгоритм машинного обучения,\n",
    "    основанный на теореме Байеса. Он используется для классификации объектов на основе комбинации\n",
    "    априорной вероятности и правдоподобия условия.\n",
    "\n",
    "    В машинном обучении наивный Байесовский классификатор является простым и эффективным алгоритмом,\n",
    "    особенно для категориальных и текстовых данных. Он основывается на предположении о независимости\n",
    "    между признаками, то есть каждый признак вносит независимый вклад в прогноз классификации объекта.\n",
    "\n",
    "    lambda_: Параметр сглаживания Лапласа, который используется для предотвращения вероятности,\n",
    "    равной нулю.\n",
    "    '''\n",
    "    timeNB_start = time.time()\n",
    "    model = NaiveBayes.train(trainingData, 1.0)\n",
    "    timeNB_end = time.time()\n",
    "    time_nb += timeNB_end - timeNB_start\n",
    "     # Прогнозирование и проверка точности\n",
    "    predictionAndLabel = testData.map(lambda p: (model.predict(p.features), p.label))\n",
    "    accuracy = 1.0 * predictionAndLabel.filter(lambda vp: vp[0] != vp[1]).count() / testData.count()\n",
    "    list_error_nb.append(accuracy)\n",
    "\n",
    "print(\"Time Random Forest \", time_rf)\n",
    "print(\"Average Error Random Forest \")\n",
    "print(reduce(lambda x, y: x + y, list_error_rf) / len(list_error_rf))\n",
    "\n",
    "\n",
    "print(\"Time LogisticRegression \", time_lr)\n",
    "print(\"Average Error Logistic Regression\")\n",
    "print(reduce(lambda x, y: x + y, list_error_lr) / len(list_error_lr))\n",
    "\n",
    "\n",
    "print(\"Time DecisionTree \", time_dt)\n",
    "print(\"Average Error Decision Tree\")\n",
    "print(reduce(lambda x, y: x + y, list_error_dt) / len(list_error_dt))\n",
    "\n",
    "\n",
    "print(\"Time Naive Bayes \", time_nb)\n",
    "print(\"Average Error Naive Bayes\")\n",
    "print(reduce(lambda x, y: x + y, list_error_nb) / len(list_error_nb))\n",
    "\n",
    "\n",
    "print(\"RF: \")\n",
    "#####################################      RF\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint(row[0], row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint(row[0], row[1:]))\n",
    "time_s = time.time()\n",
    "model = RandomForest.trainClassifier(train_rdd, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                       numTrees=2000, featureSubsetStrategy=\"auto\",\n",
    "                                       impurity='gini', maxDepth=5, maxBins=32)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time Random Forest \",  time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "print(\"LR:\")\n",
    "####################################  LR\n",
    "\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = LogisticRegressionWithLBFGS.train(train_rdd,numClasses=25)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "labels_and_predictions = test_rdd.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "\n",
    "\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "\n",
    "print(\"DT:\")\n",
    "########################################################### DT\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = DecisionTree.trainClassifier(train_rdd, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "print(\"NB:\")\n",
    "####################################### NB\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "test_rdd = test_df.rdd.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = NaiveBayes.train(train_rdd, 1.0)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "labels_and_predictions = test_rdd.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1462,
   "id": "9acd32eb",
   "metadata": {
    "cellId": "w79k9mgybxeo1ucxiyl1o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PythonRDD[326] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[327] at RDD at PythonRDD.scala:53]\n",
      "[PythonRDD[325] at RDD at PythonRDD.scala:53, PythonRDD[326] at RDD at PythonRDD.scala:53]\n",
      "Time Random Forest  59.64871859550476\n",
      "Average Error Random Forest \n",
      "0.48753411978612404\n",
      "Time LogisticRegression  17.92341113090515\n",
      "Average Error Logistic Regression\n",
      "0.48753411978612404\n",
      "Time DecisionTree  2.0962634086608887\n",
      "Average Error Decision Tree\n",
      "0.7020941008279739\n",
      "Time Naive Bayes  5.224005699157715\n",
      "Average Error Naive Bayes\n",
      "0.3610948413948317\n",
      "RF: \n",
      "Доля верных предсказаний: 81.353591160221%\n",
      "Time Random Forest  39.391579151153564\n",
      "Precision = 0.8470208622233535\n",
      "Recall = 0.8135359116022098\n",
      "F1-score = 0.8169610842219016\n",
      "accuracy: = 0.81353591160221\n",
      "Precision = 0.8470208622233535\n",
      "Recall = 0.8135359116022098\n",
      "F1-score = 0.8169610842219016\n",
      "accuracy: = 0.81353591160221\n",
      "LR:\n",
      "Доля верных предсказаний: 91.76136363636364%\n",
      "Time LogisticRegression  10.885711431503296\n",
      "Precision = 0.9187119223918171\n",
      "Recall = 0.9176136363636364\n",
      "F1-score = 0.9177221222080588\n",
      "accuracy: = 0.9176136363636364\n",
      "DT:\n",
      "Доля верных предсказаний: 72.34352256186317%\n",
      "Time LogisticRegression  0.9448306560516357\n",
      "Precision = 0.8108738598546348\n",
      "Recall = 0.7234352256186317\n",
      "F1-score = 0.7390065896686538\n",
      "accuracy: = 0.7234352256186317\n",
      "NB:\n",
      "Доля верных предсказаний: 70.22556390977444%\n",
      "Time LogisticRegression  0.6277906894683838\n",
      "Precision = 0.7207040047764555\n",
      "Recall = 0.7022556390977444\n",
      "F1-score = 0.6988517487136022\n",
      "accuracy: = 0.7022556390977444\n"
     ]
    }
   ],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova\n",
    "\n",
    "\n",
    "list_error_rf=[]\n",
    "list_error_lr=[]\n",
    "list_error_dt=[]\n",
    "subject_rdd_list = subject_rdd_list[0:3]\n",
    "\n",
    "time_rf = 0\n",
    "time_dt = 0\n",
    "time_nb = 0\n",
    "time_lr = 0\n",
    "for i in range(len(subject_rdd_list)):\n",
    "    test_subject=subject_rdd_list[i]\n",
    "    # Cоздаем новый список, который не включает в себя i-ый элемент\n",
    "    training_subjects = subject_rdd_list[:i] + subject_rdd_list[i+1:]\n",
    "    #Объединяем все RDD-объекты, кроме i-ого, в один RDD-объект\n",
    "    dataset_subject_without_missing_value=sc.union(training_subjects)\n",
    "    print(training_subjects)\n",
    "    # Загружаем и парсим файл данных в RDD-объекты типа LabeledPoint\n",
    "\n",
    "    data = dataset_subject_without_missing_value.map(parsePoint)\n",
    "    # Разделяем данные на обучающую и тестовую выборки (30% данных оставляем на тестирование)\n",
    "    trainingData = data # Все объекты, кроме i-го, используем для обучения\n",
    "    testData = test_subject.map(parsePoint)  # Используем i-й объект для тестирования\n",
    "\n",
    "    ################################################################# MODEL #############################################################################\n",
    "    ###########Train a RandomForest model.######################\n",
    "    '''\n",
    "    Случайный лес (Random Forest) - это алгоритм машинного обучения, который используется\n",
    "    для задач классификации, регрессии и кластеризации.\n",
    "    Он состоит из множества решающих деревьев, которые обучаются на подвыборках данных\n",
    "    и подмножествах признаков. Во время обучения каждое дерево строится независимо\n",
    "    от других деревьев, и каждое дерево получает случайный поднабор данных. После обучения,\n",
    "    классификация новых данных происходит путем применения каждого дерева к новым данным и выбора\n",
    "    класса с наибольшим количеством голосов.\n",
    "    \n",
    "    \n",
    "    \n",
    "    numClasses: количество классов, на которые нужно разделить данные.\n",
    "    categoricalFeaturesInfo: информация о категориальных признаках. Пустое значение означает, что все признаки являются непрерывными.\n",
    "    numTrees: количество деревьев в лесу. В данном случае равно 2000.\n",
    "    featureSubsetStrategy: стратегия выбора подмножества признаков для каждого дерева. Установлено значение \"auto\", которое позволяет алгоритму выбрать подходящую стратегию.\n",
    "    impurity: критерий для оценки качества разделения узлов в деревьях. Установлено значение 'gini'.\n",
    "    maxDepth: максимальная глубина деревьев. Установлено значение 5.\n",
    "    maxBins: максимальное количество корзин для дискретизации признаков. Установлено значение 32.\n",
    "    '''\n",
    "    timeRF_start = time.time()\n",
    "    model = RandomForest.trainClassifier(trainingData, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                       numTrees=2000, featureSubsetStrategy=\"auto\",\n",
    "                                       impurity='gini', maxDepth=5, maxBins=32)\n",
    "    timeRF_end = time.time()\n",
    "    time_rf += timeRF_end - timeRF_start\n",
    "    # Оценка модели на тестовых данных и вычисление ошибки тестирования\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    \n",
    "    testErr = labelsAndPredictions.filter(lambda vp: vp[0] != vp[1]).count()  / float(testData.count())\n",
    "    list_error_rf.append(testErr) \n",
    "    #Save the error on the leave one out \n",
    "    '''\n",
    "    \"Leave one out\" (LOO) - это метод кросс-валидации, используемый для оценки производительности\n",
    "    моделей машинного обучения. Он заключается в том, что одно наблюдение удаляется из обучающего\n",
    "    набора данных, модель обучается на оставшихся данных и затем используется для прогнозирования\n",
    "    значения удаленного наблюдения. Этот процесс повторяется для всех наблюдений в обучающем наборе,\n",
    "    и каждый раз ошибка прогнозирования рассчитывается для удаленного наблюдения.\n",
    "    list_error_rf.append(testErr) \n",
    "    '''\n",
    "    #print('Test Error = ' + str(testErr))\n",
    "    ########### Logistic Regression model.#######\n",
    "    '''\n",
    "    Logistic regression (логистическая регрессия) - это метод машинного обучения, который применяется\n",
    "    для задач классификации. Он использует логистическую функцию, чтобы преобразовать\n",
    "    линейную комбинацию входных признаков в вероятность отнесения объекта к определенному классу.\n",
    "\n",
    "    В данном коде происходит обучение модели логистической регрессии с помощью метода оптимизации LBFGS\n",
    "    (Limited-memory Broyden–Fletcher–Goldfarb–Shanno).\n",
    "    Этот метод используется для решения задач оптимизации без ограничений с помощью градиентного спуска.\n",
    "\n",
    "    Параметры:\n",
    "    trainingData - обучающая выборка;\n",
    "    numClasses - число классов в задаче классификации;\n",
    "    '''\n",
    "    timeLR_start = time.time()\n",
    "    model_regression = LogisticRegressionWithLBFGS.train(trainingData,numClasses=25)\n",
    "    timeLR_end = time.time()\n",
    "    time_lr += timeLR_end - timeLR_start\n",
    "    # Оценка модели на тестовых данных\n",
    "    labelsAndPreds = testData.map(lambda p: (p.label, model_regression.predict(p.features)))\n",
    "    trainErr = labelsAndPreds.filter(lambda vp: vp[0] != vp[1]).count() / float(data.count())\n",
    "    # Сохранение ошибки на leave one out\n",
    "    list_error_lr.append(testErr)\n",
    "    #print(\"Training Error = \" + str(trainErr))\n",
    "    \n",
    "    ########### Decision Tree .######################\n",
    "    '''\n",
    "    Decision Tree (дерево решений) - это один из самых популярных алгоритмов машинного обучения,\n",
    "    который используется как для задач классификации, так и для задач регрессии.\n",
    "    Он представляет собой структуру дерева, где каждый узел представляет собой тест на определенный\n",
    "    признак, каждое ребро соответствует значению этого признака,\n",
    "    а каждый лист соответствует конкретному классу или значению целевой переменной.\n",
    "    \n",
    "    Параметры:\n",
    "\n",
    "    numClasses: число уникальных классов в целевой переменной.\n",
    "    categoricalFeaturesInfo: словарь, который содержит информацию о категориальных признаках. По умолчанию все признаки считаются непрерывными.\n",
    "    impurity: функция, которая используется для вычисления неопределенности узла. Возможные значения - \"gini\" для индекса Джини и \"entropy\" для энтропии.\n",
    "    maxDepth: максимальная глубина дерева. Более глубокое дерево может привести к переобучению, тогда как более неглубокое дерево может привести к недообучению.\n",
    "    maxBins: максимальное количество корзин, используемых для дискретизации непрерывных признаков.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Пустые categoricalFeaturesInfo указывают на то, что все признаки являются непрерывными.\n",
    "    timeDT_start = time.time()\n",
    "    model = DecisionTree.trainClassifier(trainingData, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "    timeDT_end = time.time()\n",
    "    time_dt += timeDT_end - timeDT_start\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    testErr = labelsAndPredictions.filter(lambda vp: vp[0] != vp[1]).count() / float(testData.count())\n",
    "   # Оценка модели на тестовых данных и вычисление ошибки\n",
    "    list_error_dt.append(testErr)\n",
    "    #print('Test Error = ' + str(testErr))\n",
    "    #print(model.toDebugString())\n",
    "    \n",
    "    \n",
    "########### Naives Bayes .######################\n",
    "# Список RDD, содержащих данные по различным предметам\n",
    "subject_rdd_list=[subject1,subject2,subject3,subject4,subject5,subject6, subject7, subject8]\n",
    "list_error_nb=[]\n",
    "# timeNB_start = []\n",
    "# timeNB_end = []\n",
    "for i in range(len(subject_rdd_list)):\n",
    "    subject_rdd_list=[subject1,subject2,subject3,subject4,subject5,subject6, subject7, subject8]\n",
    "    # Извлечение тестовых данных (одного предмета)\n",
    "    test_subject=subject_rdd_list[i].map(lambda x: (map(abs, x)))\n",
    "    del subject_rdd_list[i] # Исключение тестовых данных из списка RDD\n",
    "    #Объединение оставшихся RDD, чтобы сформировать обучающий набор данных\n",
    "    dataset_subject_without_missing_value=sc.union(subject_rdd_list)\n",
    "    # Загрузка и парсинг файлов с данными в RDD объект LabeledPoint\n",
    "    data = dataset_subject_without_missing_value.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "    trainingData = data \n",
    "    testData = test_subject.map(parsePoint)\n",
    "    ########## Naive Bayes model.######################\n",
    "    '''\n",
    "    Наивный Байесовский классификатор - это вероятностный алгоритм машинного обучения,\n",
    "    основанный на теореме Байеса. Он используется для классификации объектов на основе комбинации\n",
    "    априорной вероятности и правдоподобия условия.\n",
    "\n",
    "    В машинном обучении наивный Байесовский классификатор является простым и эффективным алгоритмом,\n",
    "    особенно для категориальных и текстовых данных. Он основывается на предположении о независимости\n",
    "    между признаками, то есть каждый признак вносит независимый вклад в прогноз классификации объекта.\n",
    "\n",
    "    lambda_: Параметр сглаживания Лапласа, который используется для предотвращения вероятности,\n",
    "    равной нулю.\n",
    "    '''\n",
    "    timeNB_start = time.time()\n",
    "    model = NaiveBayes.train(trainingData, 1.0)\n",
    "    timeNB_end = time.time()\n",
    "    time_nb += timeNB_end - timeNB_start\n",
    "     # Прогнозирование и проверка точности\n",
    "    predictionAndLabel = testData.map(lambda p: (model.predict(p.features), p.label))\n",
    "    accuracy = 1.0 * predictionAndLabel.filter(lambda vp: vp[0] != vp[1]).count() / testData.count()\n",
    "    list_error_nb.append(accuracy)\n",
    "\n",
    "print(\"Time Random Forest \", time_rf)\n",
    "print(\"Average Error Random Forest \")\n",
    "print(reduce(lambda x, y: x + y, list_error_rf) / len(list_error_rf))\n",
    "\n",
    "\n",
    "print(\"Time LogisticRegression \", time_lr)\n",
    "print(\"Average Error Logistic Regression\")\n",
    "print(reduce(lambda x, y: x + y, list_error_lr) / len(list_error_lr))\n",
    "\n",
    "\n",
    "print(\"Time DecisionTree \", time_dt)\n",
    "print(\"Average Error Decision Tree\")\n",
    "print(reduce(lambda x, y: x + y, list_error_dt) / len(list_error_dt))\n",
    "\n",
    "\n",
    "print(\"Time Naive Bayes \", time_nb)\n",
    "print(\"Average Error Naive Bayes\")\n",
    "print(reduce(lambda x, y: x + y, list_error_nb) / len(list_error_nb))\n",
    "\n",
    "\n",
    "print(\"RF: \")\n",
    "#####################################      RF\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint(row[0], row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint(row[0], row[1:]))\n",
    "time_s = time.time()\n",
    "model = RandomForest.trainClassifier(train_rdd, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                       numTrees=2000, featureSubsetStrategy=\"auto\",\n",
    "                                       impurity='gini', maxDepth=5, maxBins=32)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time Random Forest \",  time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "print(\"LR:\")\n",
    "####################################  LR\n",
    "\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = LogisticRegressionWithLBFGS.train(train_rdd,numClasses=25)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "labels_and_predictions = test_rdd.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "\n",
    "\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "\n",
    "print(\"DT:\")\n",
    "########################################################### DT\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "test_rdd = test_df.rdd.map(lambda row : LabeledPoint((row[0]), row[1:]))\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = DecisionTree.trainClassifier(train_rdd, numClasses=25, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "\n",
    "\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "print(\"NB:\")\n",
    "####################################### NB\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "test_subject_df = sc.union(subject_rdd_list).toDF(load_IMU_noTS())\n",
    "\n",
    "train_df, test_df = test_subject_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "train_rdd = train_df.rdd.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "test_rdd = test_df.rdd.map(lambda x: (map(abs, x))).map(parsePoint)\n",
    "\n",
    "\n",
    "\n",
    "time_s = time.time()\n",
    "model = NaiveBayes.train(train_rdd, 1.0)\n",
    "time_e = time.time()\n",
    "\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "labels_and_predictions = test_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "matched_predictions_cont = labels_and_predictions.filter(lambda lp: lp[0] == lp[1]).count()\n",
    "accuracy_percent = matched_predictions_cont * 100 / float(test_rdd.count())\n",
    "print(\"Доля верных предсказаний:\", str(accuracy_percent) + \"%\")\n",
    "print(\"Time LogisticRegression \", time_e - time_s)\n",
    "#print(\"Структура обученной модели:\")\n",
    "#print(model.toDebugString())\n",
    "labels_and_predictions = test_rdd.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "\n",
    "metrics = None\n",
    "metrics = MulticlassMetrics(labels_and_predictions)\n",
    "\n",
    "# Вычисление метрик качества модели\n",
    "\n",
    "precision = metrics.weightedPrecision\n",
    "recall = metrics.weightedRecall\n",
    "f1Score = metrics.weightedFMeasure()\n",
    "accuracy = metrics.accuracy\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1-score = %s\" % f1Score)\n",
    "print(\"accuracy: = %s\" % accuracy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7efab",
   "metadata": {
    "cellId": "xyufbd0wqe9m0dpy35fbeq"
   },
   "outputs": [],
   "source": [
    "#!spark --cluster bigdata-course-spark-cluster-jdrtk8xm --session gr0373-Vlasyuk-Serebryakova"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "4776fbd5-f3c9-48a1-ac31-cff6c58b8003",
  "notebookPath": "PAMAP2_analys.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
